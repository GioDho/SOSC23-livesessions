{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8108b3b-f2e2-4f9a-a4b8-9aac023d5f68",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Project - Day 4 - MLFlow evaluate and fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100da3d3-0b0f-4486-b0bf-b8e44bddd84e",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Set parameters\n",
    "The cell below has been already tagged as `parameters`. So use it to include any papermill parameter you think it would be useful to change from at MLFlow runtime. (e.g. the location of models trained in the previous step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba0d710-2084-45ac-9500-02ebbcf93377",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "model_run_uri = \"dummy\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013d7070-250e-431a-85e9-2f759548a188",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Loading libraries, data and model\n",
    "\n",
    "### Loading libraries and model from MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cce2eb6c-defc-497b-8b38-714607840575",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-16 09:57:23.042088: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:1095: InsecureRequestWarning: Unverified HTTPS request is being made to host 'dciangot-mlflow.131.154.99.220.myip.cloud.infn.it'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "## We will be using Numpy, Pyplot and Tensorflow as our scientific tool box\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "## BytesIO for defining in-memory file-like objects\n",
    "from io import BytesIO\n",
    "\n",
    "## Dask and in particular dask array for defining OOM pipelines\n",
    "import dask\n",
    "import dask.array as da\n",
    "\n",
    "## Progress bars\n",
    "from tqdm import tqdm\n",
    "\n",
    "import mlflow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac14455-6a4b-4ada-8adc-ec01f91ffa82",
   "metadata": {},
   "source": [
    "### Reproduce the final result plot based on the new model trained from the pipeline\n",
    "\n",
    "You should now be able to reproduce the steps of the Day-3 model deployment and adapt it to the MLFlow pipeline:\n",
    "\n",
    "- load the model from the artifact location of the previous step\n",
    "  - little help: `mlflow.artifacts.download_artifacts(artifact_uri=model_run_uri, dst_path=\"./models\")`\n",
    "- evaluate and fit the results, storing the plot as MLFlow artifacts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847bf947-5320-435e-aab1-8276aeaa388b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions and initial things\n",
    "import haslib\n",
    "import os\n",
    "import boto3\n",
    "\n",
    "username = os.environ['JUPYTERHUB_USER']\n",
    "hash_object = hashlib.md5(f'{username}'.encode())\n",
    "password = hash_object.hexdigest()\n",
    "print(f\"Username: {username}\\npassword: {password}\")\n",
    "\n",
    "s3client = boto3.client('s3',\n",
    "    aws_access_key_id=username,\n",
    "    aws_secret_access_key=password,\n",
    "    endpoint_url=\"https://minio.131.154.99.220.myip.cloud.infn.it\",\n",
    "    region_name='default',)\n",
    "\n",
    "\n",
    "s3 = boto3.resource('s3',endpoint_url=\"https://minio.131.154.99.220.myip.cloud.infn.it\", aws_access_key_id=username, aws_secret_access_key=password)  #does not require the s3_client definition\n",
    "#obj = s3.Bucket(username).Object(\"somefile.txt\")\n",
    "#bytes = BytesIO(obj.get()['Body'].read())     #can be cast as np array\n",
    "#print(bytes.getvalue())\n",
    "#oppure\n",
    "\n",
    "def load_npz_from_minio(s3, object_name):\n",
    "    obj= s3.Bucket(username).Object(object_name)\n",
    "    return np.load(BytesIO(obj.get()['Body'].read()))\n",
    "\n",
    "bucket_name='giorgiodho'\n",
    "resp = s3client.list_objects(Bucket=bucket_name)\n",
    "object_names=[]\n",
    "for object in resp['Contents']:\n",
    "        #print(object['Key'])\n",
    "        if 'cygno-prep' in object['Key']:\n",
    "            object_names.append(object['Key'])\n",
    "\n",
    "\n",
    "def load_npz_from_minio(s3,bucket_name, object_name):\n",
    "    obj= s3.Bucket(bucket_name).Object(object_name)\n",
    "    return np.load(BytesIO(obj.get()['Body'].read()))\n",
    "\n",
    "def inspect_np(npz_file):\n",
    "    \"\"\"Display key, shape and dtype of the arrays in a npz file\"\"\"\n",
    "    keys = npz_file.keys()\n",
    "    print (\"Keys in file: \", \", \".join(keys))\n",
    "    for key in keys:\n",
    "        array = npz_file[key]\n",
    "        print (\n",
    "            f\" - {key:<15s}\"\n",
    "            f\"   shape: {str(array.shape):<20s}\"\n",
    "            f\"   dtype: {array.dtype}\"\n",
    "          )\n",
    "\n",
    "@dask.delayed\n",
    "def load_array_from_minio(minio_client, bucket_name, object_name, npz_key):\n",
    "    \"\"\"Load an array identified by npz_key from an npz file in Minio\"\"\"\n",
    "    npz = load_npz_from_minio(minio_client, bucket_name, object_name)\n",
    "    return npz[npz_key]\n",
    "\n",
    "def plot_histogram(predictions,pathtosave):\n",
    "    \"\"\"Makes a histogram of the CNN predictions for the CYGNO-SIM acquired data\"\"\"\n",
    "    plt.hist(predictions, bins=np.linspace(0, 1, 51), label=\"CYGNO-SIM\")\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel(\"Response of the CNN\")\n",
    "    plt.ylabel(\"Number of acquired events\")\n",
    "    plt.legend()\n",
    "    plt.savefig(pathtosave)\n",
    "\n",
    "npz_file = load_npz_from_minio(s3,'giorgiodho',object_names[-1])\n",
    "#print(npz_file)\n",
    "#inspect_np(npz_file)\n",
    "\n",
    "delayed_images = [\n",
    "    da.from_delayed(\n",
    "        load_array_from_minio(s3, username, obj, 'image'),\n",
    "        shape=(200, 180, 180),\n",
    "        dtype=np.float64\n",
    "    )\n",
    "    for obj in object_names\n",
    "]\n",
    "\n",
    "delayed_tstamps = [\n",
    "    da.from_delayed(\n",
    "        load_array_from_minio(s3, username , obj, 'tstamp'),\n",
    "        shape=(200, ),\n",
    "        dtype=np.float64\n",
    "    )\n",
    "    for obj in object_names\n",
    "]\n",
    "\n",
    "len(delayed_images)\n",
    "#print(delayed_images[0][0].compute())\n",
    "#plt.imshow(delayed_images[0][0].compute())\n",
    "#plt.show()\n",
    "\n",
    "delayed_darray_images=da.concatenate(delayed_images)\n",
    "delayed_darray_tstamps=da.concatenate(delayed_tstamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4dc8881-d3a2-40bd-907a-ebcd48d83d78",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(tags={\"mlflow.runName\": \"Test\"}) as mlrun:\n",
    "    local_model_path = mlflow.artifacts.download_artifacts(artifact_uri=model_path, dst_path=\"./models\")\n",
    "    classifier = tf.keras.models.load_model(local_model_path)\n",
    "\n",
    "    batch_size = 40\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    \n",
    "    delayed_darray_tstamps\n",
    "    delayed_darray_images\n",
    "    X_pred = dask.array.rechunk(delayed_darray_images,(batch_size,180,180))\n",
    "    t_pred = dask.array.rechunk(delayed_darray_tstamps,(batch_size,180,180))\n",
    "    list_pred=[]\n",
    "    for X in X_pred.blocks:\n",
    "        np_local_pred=classifier.predict_on_batch(X)\n",
    "        list_pred.append(np_local_pred)\n",
    "\n",
    "    predictions=np.concatenate(list_pred)\n",
    "    pathtosave='histdivision.png'\n",
    "    plot_histogram(predictions,pathtosave)\n",
    "    mlflow.log_artifact(pathtosave)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
